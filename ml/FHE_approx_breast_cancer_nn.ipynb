{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FHE_approx_breast_cancer_nn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uN3JcJ27hhU"
      },
      "source": [
        "# Adapted from https://neptune.ai/blog/neural-network-guide\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx2okwiG7lFf"
      },
      "source": [
        "data_dir = \"/content/gdrive\"\n",
        "folder_name = \"kaggle_dataaset\"\n",
        "image_folders = os.path.join(data_dir, folder_name)\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((50, 50)), transforms.ToTensor()])\n",
        "images = []\n",
        "for file in os.listdir(image_folders):\n",
        "    try:\n",
        "      images.append(ImageFolder(os.path.join(image_folders, file), transform=transform))\n",
        "    except:\n",
        "      print(file)\n",
        "datasets = torch.utils.data.ConcatDataset(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgV5Iyok7myk"
      },
      "source": [
        "i = 0\n",
        "for dataset in datasets.datasets:\n",
        "    if not i:\n",
        "        result = Counter(dataset.targets)\n",
        "        i += 1\n",
        "    else:\n",
        "        result += Counter(dataset.targets)\n",
        "\n",
        "result = dict(result)\n",
        "print(\"\"\"Total Number of Images for each Class:\n",
        "    Class 0 (No Breast Cancer): {}\n",
        "    Class 1 (Breast Cancer present): {}\"\"\".format(result[0], result[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwEVGEYD7o9O"
      },
      "source": [
        "random_seed = 189 # the CS version\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "test_size = int(0.25 * (result[0] + result[1]))\n",
        "print(test_size)\n",
        "train_size = len(datasets) - test_size\n",
        "train_dataset, test_dataset = random_split(datasets, [train_size, test_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX3flOT77zS-"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwNDv6iN70lY"
      },
      "source": [
        "# functions to show an image\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[:6], nrow=3))\n",
        "# show labels\n",
        "labels[:6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zgLO33F717r"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq4TgPeI8MZb"
      },
      "source": [
        "### CalHacks 8.0 FHE\n",
        "Using Taylor series expansion of smooth ReLU approximation (Softplus) of:\n",
        "\n",
        "$$\n",
        "f(x) = \\ln{(1+e^x)} \\\\\n",
        "\\approxeq \\ln{2} + \\frac{x}{2} + \\frac{x^2}{8} + O(x^4)\n",
        "$$.\n",
        "\n",
        "And smooth maximum approximation:\n",
        "\n",
        "$$\n",
        "\\max{\\textbf{x}} = \\frac{\\sum_{x_i \\in \\textbf{x}} x_i^{\\left(p+k\\right)}}{\\sum_{x_i \\in \\textbf{x}} x^p}\n",
        "$$\n",
        "for large constant $p, k \\in \\mathbb{Z}^+$.\n",
        "\n",
        "Above approximations chosen to ensure that all operations are within the subset allowed for Fully-Homomorphic Encryption (FHE). We inject them into `pytorch` here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr4cgAqU-23P"
      },
      "source": [
        "from fhe_approx import patch_relu, patch_maxpools\n",
        "patch_relu(F, nn)\n",
        "patch_maxpools(F, nn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoBOAZrV_WcK"
      },
      "source": [
        "### FHE for log-softmax\n",
        "To handle log-softmax with the subset of operations permitted in FHE context, we approximate $\\log{x}$ using Taylor series expansion with radius 2 about origin as $$\\ln{x} \\approxeq (x - 1) - \\frac{1}{2} \\left(x-1\\right)^2 +\\frac{1}{3}\\left(x-1\\right)^3 - \\frac{1}{4} \\left(x-1\\right)^4 + o(x^5)$$.\n",
        "\n",
        "We similarly handle the exponential function using the Maclaurin Series as $$e^x \\triangleq \\exp{x} = \\sum_{i=0}^{\\infty} \\frac{x^i}{i!} \\\\ \\approxeq 1 + \\frac{x!}{1} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + o(x^5)$$. \n",
        "\n",
        "Putting these approximations together, we can handle softmax, which is defined as $$\n",
        "s(\\textbf{x}) _i \\triangleq \\frac{e^{z_i}}{\\sum_{x_i \\in \\textbf{x}} e^{z_i}}\n",
        "$$\n",
        "for $\\textbf{z} \\in \\mathbb{R}^k$. Using the exponential function approximation above, it is tractable in the FHE context.\n",
        "\n",
        "Finally, we simply take the natural log of the output of the above using the log approximation we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tTOMmvgC6_s"
      },
      "source": [
        "from fhe_approx import patch_logsoftmax\n",
        "patch_logsoftmax(F, nn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6AR5LSY73dy"
      },
      "source": [
        "class BreastCancerClassifyNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BreastCancerClassifyNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "    self.conv3 = nn.Conv2d(128, 256, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(4096, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 512)\n",
        "    self.fc3 = nn.Linear(512, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.pool(F.relu(self.conv3(x)))\n",
        "    x = x.view(-1, self.flat_features(x))\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x)\n",
        "    return x\n",
        "\n",
        "  def flat_features(self, x):\n",
        "    size = x.size()[1:]\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features\n",
        "\n",
        "net = BreastCancerClassifyNet()\n",
        "net = net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPoC6cL-75Tw"
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr = 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tUWdYXT76gl"
      },
      "source": [
        "test_data_iter = iter(testloader)\n",
        "test_images, test_labels = test_data_iter.next()\n",
        "for epoch in range(20):\n",
        "  running_loss = 0\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    input_imgs, labels = data\n",
        "    input_imgs = input_imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(input_imgs)\n",
        "    labels = labels.unsqueeze(1).float()\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #printing stats and checking prediction as we train\n",
        "    running_loss += loss.item()\n",
        "    if i % 10000 == 0:\n",
        "      print('epoch', epoch+1, 'loss', running_loss/10000)\n",
        "      imshow(torchvision.utils.make_grid(test_images[0].detach()))\n",
        "      test_out = net(test_images.to(device))\n",
        "      _, predicted_out = torch.max(test_out, 1)\n",
        "      print('Predicted : ', ' '.join('%5s' % predicted_out[0]))\n",
        "\n",
        "\n",
        "print('Training finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFY77gM078Og"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    test_images, test_labels = data\n",
        "    test_out = net(test_images.to(device))\n",
        "    _, predicted = torch.max(test_out.data, 1)\n",
        "    total += test_labels.size(0)\n",
        "    for _id, out_pred in enumerate(predicted):\n",
        "      if int(out_pred) == int(test_labels[_id]):\n",
        "        correct += 1\n",
        "\n",
        "print('Accuracy of the network on the 44252 test images: %d %%' % (\n",
        "        100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4XMN4Wa79us"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}